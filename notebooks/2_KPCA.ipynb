{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook, we introduce kernel-based versions of the models covered in the [Linear Methods](1_PCA_MDS_PCR.ipynb) notebook.\n",
    "\n",
    "As always, for each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. Second, we will employ a \"Sklearn/Skcosmo Class\" for the models, which can be found in the skcosmo/sklearn module and contains all necessary functions.\n",
    "\n",
    "**Note**: To display values of variables in the markdown cells, you should [enable the jupyter contrib nbextension](https://stackoverflow.com/questions/52812231/print-variable-in-jupyter-notebook-markdown-cell-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append(\"../\")\n",
    "from utilities.general import load_variables, sorted_eig, get_stats\n",
    "from utilities.plotting import (\n",
    "    plot_projection,\n",
    "    plot_regression,\n",
    "    check_mirrors,\n",
    "    get_cmaps,\n",
    "    table_from_dict,\n",
    ")\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from functools import partial\n",
    "\n",
    "from skcosmo.decomposition import KernelPCovR\n",
    "from skcosmo.preprocessing import KernelNormalizer\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use(\"../utilities/kernel_pcovr.mplstyle\")\n",
    "dbl_fig = (2 * plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb). As mentioned in the [foreword notebook](0_Foreword.ipynb), pre-computed features should be downloaded from [here](https://www.dropbox.com/s/itokckbbkvxaqsk/precomputed.npz?dl=0) and the file `precomputed.npz` should be copied to the `datasets/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Kernels\n",
    "\n",
    "## The Kernel Trick\n",
    "\n",
    "Many kernel methods are similar to linear methods,\n",
    "except that we take advantage of the [**kernel trick**](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick) in order to \n",
    "introduce a non-linear transformation of the feature space.\n",
    "\n",
    "The kernel trick consists in introducing a [**positive definite**](https://en.wikipedia.org/wiki/Positive-definite_kernel) function of pairs of samples, $ k(\\mathbf{x}, \\mathbf{x}^{\\prime})$, that defines implicitly a higher (possibly infinite) dimensional feature space, in which one can apply linear analysis methods. \n",
    "\n",
    "There are [many](https://en.wikipedia.org/wiki/Positive-definite_kernel#Examples_of_p.d._kernels) positive definite kernels. Common kernels are the linear (dot product) kernel,\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\mathbf{x}^T \\mathbf{x},\n",
    "\\end{equation}\n",
    "and the Gaussian kernel,\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\exp{\\left(-\\gamma\\lVert \\mathbf{x} - \\mathbf{x}^{\\prime}\\rVert^2\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "In this notebook we use the gaussian kernel (```rbf_kernel(XA, XB, gamma=1.0)```), however, we have also included the linear kernel function (```linear_kernel(XA, XB)```) in ```sklearn```. You can check rather easily that if you use a linear kernel all of the kernel methods reduce explicitly to linear regression or principal component analysis in the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the kernels in this cell will change them throughout the notebook tutorial.\n",
    "# The function variable designates the kernel function to use throughout the notebook.\n",
    "# The string variable is used to pass to the utility classes.\n",
    "kernel_params = {\"kernel\": \"rbf\", \"gamma\": 1.0}\n",
    "kernel_func = partial(rbf_kernel, gamma=1.0)\n",
    "kernel_type = \"gaussian\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [representer theorem](https://en.wikipedia.org/wiki/Representer_theorem) guarantees that if the kernel is a positive definite function (both these two examples are) there is a Hilbert space with elements $\\phi(\\mathbf{x})$ whose dot product reproduces the kernel, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\phi(\\mathbf{x})^T\\phi(\\mathbf{x}^{\\prime}).\n",
    "\\end{equation}\n",
    "\n",
    "This Hilbert space is known as the [**reproducing kernel Hilbert space**](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space), or RKHS. If one builds a kernel matrix $\\mathbf{K}$ whose elements contain the kernel computed between the corresponding pair of samples, \n",
    "For simple kernels, such as the linear kernel shown above, the RKHS is intuitive: $\\phi(\\mathbf{x}) = \\mathbf{x}$.\n",
    "However, for even slightly more complex kernel functions, the RKHS may be infinitely dimensional, such as the [RBF kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel).\n",
    "If one builds a kernel matrix $\\mathbf{K}$ whose elements contain the kernel computed between the corresponding pair of samples, \n",
    "\n",
    "\\begin{equation}\n",
    "    K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "\\end{equation}\n",
    "\n",
    "the formal relation with the reproducing features can be written in a matrix notation\n",
    "\n",
    "\\begin{equation}\n",
    "\\quad \\mathbf{K} = \\mathbf{\\Phi_X\\Phi_X^T}.\n",
    "\\end{equation}\n",
    "\n",
    "$\\mathbf{\\Phi_X}$ indicates a matrix that holds the values of the RKHS features for each sample in $\\mathbf{X}$, which means that $\\mathbf{X}$and $\\mathbf{\\Phi_X}$ have the same number of samples, but different numbers of features. To simplify the notation, in this notebook we drop the $\\mathbf{X}$ subscript and refer to this matrix simply as $\\mathbf{\\Phi}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train_raw = kernel_func(X_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize that even if in general $\\phi(\\mathbf{x})$ is not known, _for a given data set_ the representer theorem provides an explicit construction for an approximation of the RKHS features. The construction is very closely related to the KPCA method that we discuss below. \n",
    "\n",
    "Start by writing an eigendecomposition of the kernel matrix, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} = \\mathbf{U_K} \\mathbf{\\Lambda_K} \\mathbf{U_K}^T.\n",
    "\\end{equation}\n",
    "\n",
    "If one defines \n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi} =  \\mathbf{U_K} \\mathbf{\\Lambda_K}^{1/2} = \\mathbf{K} \\mathbf{U_K} \\mathbf{\\Lambda_K}^{-1/2}\n",
    "\\end{equation}\n",
    "one sees that $\\mathbf{\\Phi}\\mathbf{\\Phi}^T = \\mathbf{K}$: the eigenvectors of the kernel matrix make it possible to construct a set of features whose scalar product reproduces exactly the values of the kernel for the dataset. \n",
    "\n",
    "The second equality is important because it provides a recipe to build an _approximate_ set of RKHS features for a _new_ set of points. If $\\mathbf{K}_{NN}$ indicates the kernel matrix for the train set, and the matrix $\\mathbf{K}_{N'N}$ contains the kernels between some new (e.g. test-set) samples and the train samples, which means the size of $\\mathbf{K}_{N'N}$  is $n_{new\\ samples}\\times n_{train\\ sample}$,  one can compute \n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi}_{N'N} = \\mathbf{K}_{N'N} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2},\n",
    "\\end{equation}\n",
    "where $\\mathbf{U}$ and $\\mathbf{\\Lambda}$ refer to the eigendecomposition of the square $\\mathbf{K}_{NN}$.\n",
    "Then, $\\mathbf{\\Phi}_{N'N}$ is a matrix whose entries approximate the RKHS features for the new set of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_K, U_K = np.linalg.eigh(K_train_raw)\n",
    "\n",
    "# U_K/v_K are already sorted, but in *increasing* order, so reverse them\n",
    "U_K = np.flip(U_K, axis=1)\n",
    "v_K = np.flip(v_K, axis=0)\n",
    "\n",
    "U_K = U_K[:, v_K > 0]\n",
    "v_K = v_K[v_K > 0]\n",
    "\n",
    "Phi = U_K @ np.diag(np.sqrt(v_K))\n",
    "\n",
    "print(np.linalg.norm((K_train_raw - Phi @ Phi.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Centering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as it is often convenient to center features for linear methods, it is often beneficial to work with kernels that are also \"centered\". Centering a kernel can be understood in terms of centering of the associated RKHS features.  \n",
    "\n",
    "Let us first introduce the centering matrix $\\mathbf{1}_{N'N}$, which is just is a $N'\\times N$ matrix for which each element takes value $1/N$. \n",
    "\n",
    "Using the expression above for $\\mathbf{\\Phi}$, one can compute \n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{\\mathbf{\\Phi}}_{N'N} = \\mathbf{1}_{N'N} \\mathbf{K}_{NN} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "A similar centering matrix can be built for $\\mathbf{\\Phi}_{NN}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{\\mathbf{\\Phi}}_{NN} = \\mathbf{1}_{NN} \\mathbf{K}_{NN} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "The centered kernel $\\tilde{\\mathbf{K}}_{N'N}$ reads\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{K}}_{N'N} = (\\mathbf{\\Phi}_{N'N} - \\bar{\\mathbf{\\Phi}}_{N'N})(\\mathbf{\\Phi}_{NN}-\\bar{\\mathbf{\\Phi}}_{NN})^T = \n",
    "\\mathbf{K}_{N'N} -  \\mathbf{1}_{N'N} \\mathbf{K}_{NN} -  \\mathbf{K}_{N'N}\\mathbf{1}_{NN}\n",
    "+ \\mathbf{1}_{N'N} \\mathbf{K}_{NN} \\mathbf{1}_{NN}.\n",
    "\\end{equation}\n",
    "\n",
    "which can be computed without the need of ever evaluating explicitly the RKHS features.\n",
    "This is a common pattern in kernel methods: RKHS features allow casting problems in a linear\n",
    "language, but eventually the linear problem can yield an equation in which one only needs to evaluate the kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we center the kernels, which can be done with a utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_centerer = KernelNormalizer()\n",
    "K_train = k_centerer.fit_transform(K_train_raw)\n",
    "K_scale = k_centerer.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Testing Set Kernel\n",
    "\n",
    "For new data, we must generate a new kernel. Notice that a kernel takes *two* sets of data as arguments, one of which can be our testing $\\mathbf{X}_{N'}$ and the other the training $\\mathbf{X}_N$. This contains the kernels computed between all of the test set samples $N'$ and the train set samples $N$. This can be computed as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_test = kernel_func(X_test, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We center with the training kernel as reference, as discussed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centering relative to the approximate RHKS defined by\n",
    "# the training kernel matrix can be achieved specifying\n",
    "# K_train as the reference kernel matrix\n",
    "\n",
    "K_test = k_centerer.transform(K_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA (KPCA)\n",
    "\n",
    "In [kernel principal component analysis](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)\n",
    "(KPCA) we take advantage of the kernel in order to \n",
    "introduce a non-linear transformation of the feature space \n",
    "[(Scholkopf 1996)](http://www.face-rec.org/algorithms/Kernel/kernelPCA_scholkopf.pdf), \n",
    "[(Scholkopf 1998)](http://www.doi.org/10.1162/089976698300017467), and then proceed to single out the largest-variance directions in RKHS. \n",
    "\n",
    "With the linear (dot product) kernel, performing KPCA with a linear kernel is equivalent to performing standard PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principal Components\n",
    "\n",
    "KPCA proceeds analogously to PCA. First, the eigenvalues and eigenvectors of $\\mathbf{K}$ are computed:\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} = \\mathbf{U_K} \\mathbf{\\Lambda_K} \\mathbf{U_K}^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_K, U_K = np.linalg.eigh(K_train)\n",
    "\n",
    "# U_K/v_K are already sorted, but in *increasing* order, so reverse them\n",
    "U_K = np.flip(U_K, axis=1)\n",
    "v_K = np.flip(v_K, axis=0)\n",
    "\n",
    "U_K = U_K[:, v_K > 0]\n",
    "v_K = v_K[v_K > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The KPCA Projection\n",
    "\n",
    "The KPCA projection may then be computed taking the first $n_{PCA}$ components \n",
    "\\begin{equation}\n",
    "\\mathbf{T}=\\mathbf{\\hat{U}_K}\\mathbf{\\hat{\\Lambda}_K}^{1/2} = \\mathbf{\\hat{U}_K}\\mathbf{\\hat{\\Lambda}_K} (\\mathbf{\\hat{U}_K}^T\\mathbf{\\hat{U}_K}) \\mathbf{\\hat{\\Lambda}_K}^{-1/2} = \\mathbf{K} \\mathbf{\\hat{U}_K} \\mathbf{\\hat{\\Lambda}_K}^{-1/2}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\mathbf{T} = \\mathbf{K}\\mathbf{P}_{KT} = \\mathbf{K} \\mathbf{\\hat{U}_K} \\mathbf{\\hat{\\Lambda}_K}^{-1/2}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember that in PCA, the projection $\\mathbf{T} = \\mathbf{X}\\mathbf{U}_C$. So why the factor of $\\mathbf{\\Lambda}_K^{-1/2}$?**\n",
    "\n",
    "The eigenvectors of $\\mathbf{K} = \\mathbf{\\Phi}\\mathbf{\\Phi}^T$ are in fact analogous to the eigenvectors of the Gram matrix $\\mathbf{X}\\mathbf{X}^T$ and thereby related to those of the covariance $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X} = \\mathbf{U}_C \\mathbf{\\Lambda}_C \\mathbf{U}_C^T$ by $\\mathbf{X}\\mathbf{U_C}$, which is not normalized. In essence, the $\\mathbf{\\Lambda}_K^{-1/2}$ factor serves to normalize our projection matrix. [(Tipping 2001)](https://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf) \n",
    "\n",
    "Note also that the KPCA latent space corresponds to the highest-variance components in the RKHS: if one does not truncate the latent space, $\\mathbf{T}=\\mathbf{\\Phi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_PC = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = U_K[:, :n_PC] @ np.diagflat(1.0 / np.sqrt(v_K[0:n_PC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for the testing data is then identical to that for the projection of the train set:\n",
    "\\begin{equation}\n",
    "    \\mathbf{T} = \\mathbf{K}_{N'N} \\mathbf{P}_{KT}\n",
    "\\end{equation}\n",
    "$\\mathbf{P}_{KT}$ is the projection obtained during training, while the $\\mathbf{K}_{N'N}$ indicates the kernel between the test set and the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KPCA_train = K_train @ PKT\n",
    "T_KPCA_test = K_test @ PKT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if we used a linear kernel, the projection would be identical to the PCA projection (modulo reflection, since the sign of the eigenvectors is not defined), and it would likewise correspond to the Classical MDS solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "plot_projection(\n",
    "    Y_train, T_KPCA_train, fig=fig, ax=axes[0], title=\"KPCA of Training Data\", **cmaps\n",
    ")\n",
    "plot_projection(\n",
    "    Y_test, T_KPCA_test, fig=fig, ax=axes[1], title=\"KPCA of Testing Data\", **cmaps\n",
    ")\n",
    "fig.subplots_adjust(wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is not done very often (because the kernel acts in a different space than the original feature space) it is possible to reconstruct an approximation of the feature vector based on the KPCA latent space. Consider this like \"predicting\" $\\mathbf{X}$, similar to predicting $\\mathbf{Y}$ in linear regression. The projection matrix $\\mathbf{P}_{TX}$ corresponds to the least-square weights\n",
    "    \n",
    "\\begin{equation}    \n",
    "\\mathbf{P}_{TX} = (\\mathbf{T}\\mathbf{T}^T)^{-1}\\mathbf{T}^T \\mathbf{X} =  \\mathbf{\\hat{\\Lambda}}_K^{-1} \\mathbf{T}^T \\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "Where the factor of $\\mathbf{\\Lambda_K}^{-1}$ arises from the fact that\n",
    "$\\mathbf{T}^T\\mathbf{T}=\n",
    "\\mathbf{\\Lambda_K}^{-1/2}\\mathbf{U_K}^T\\mathbf{K}^T\\mathbf{K}\\mathbf{U_K}\\mathbf{\\Lambda_K}^{-1/2}=\n",
    "\\mathbf{\\Lambda_K}^{1/2}\\mathbf{U_K}^T\\mathbf{U_K}\\mathbf{\\Lambda_K}^{1/2}=\n",
    "\\mathbf{\\Lambda_K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTX = np.diagflat(1 / ((v_K[:n_PC]))) @ T_KPCA_train.T @ X_train\n",
    "\n",
    "Xr_train = T_KPCA_train @ PTX\n",
    "Xr_test = T_KPCA_test @ PTX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KPCA projection approximates the kernel matrix, so one can check for convergence using the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_approx_train = T_KPCA_train @ T_KPCA_train.T\n",
    "\n",
    "K_test_test = kernel_func(X_test, X_test)\n",
    "K_test_test = k_centerer.transform(K_test_test)\n",
    "K_approx_test = T_KPCA_test @ T_KPCA_test.T\n",
    "\n",
    "table_from_dict(\n",
    "    [\n",
    "        get_stats(\n",
    "            x=X_train,\n",
    "            xr=Xr_train,\n",
    "            y=Y_train,\n",
    "            t=T_KPCA_train,\n",
    "            k=K_train,\n",
    "            kapprox=K_approx_train,\n",
    "        ),\n",
    "        get_stats(\n",
    "            x=X_test,\n",
    "            xr=Xr_test,\n",
    "            y=Y_test,\n",
    "            t=T_KPCA_test,\n",
    "            k=K_test_test,\n",
    "            kapprox=K_approx_test,\n",
    "        ),\n",
    "    ],\n",
    "    headers=[\"Training\", \"Testing\"],\n",
    "    title=\"KPCA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in `scikit-learn`\n",
    "\n",
    "Classes from `sklearn` enable computing KPCA and KRR. Here we'll review the basics of their usage; readers are encouraged to refer to sklearn documentation for further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set kernel = 'precomputed' and supply kpca.fit(X) with X = K_train to improve computational efficiency.\n",
    "kpca = KernelPCA(n_components=2, **kernel_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpca.fit(K_train)` will generate the kernel for $\\mathbf{X}$ and compute/internally store the eigenvectors/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca.fit(K_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpca.transform(K)` will compute and return the KPCA projection $\\mathbf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KPCA_train = kpca.transform(K_train)\n",
    "T_KPCA_test= kpca.transform(K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "plot_projection(\n",
    "    Y_train, T_KPCA_train, title=\"KPCA of Training Data\", fig=fig, ax=axes[0], **cmaps\n",
    ")\n",
    "plot_projection(\n",
    "    Y_test, T_KPCA_test, title=\"KPCA of Testing Data\", fig=fig, ax=axes[1], **cmaps\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTX = np.diagflat(1 / ((v_K[:n_PC]))) @ T_KPCA_train.T @ X_train\n",
    "\n",
    "Xr_train = T_KPCA_train @ PTX\n",
    "Xr_test = T_KPCA_test @ PTX\n",
    "\n",
    "K_approx_train = T_KPCA_train @ T_KPCA_train.T\n",
    "K_test_test = kernel_func(X_test, X_test)\n",
    "K_approx_test = T_KPCA_test @ T_KPCA_test.T\n",
    "\n",
    "table_from_dict(\n",
    "    [\n",
    "        get_stats(\n",
    "            x=X_train,\n",
    "            xr=Xr_train,\n",
    "            y=Y_train,\n",
    "            t=T_KPCA_train,\n",
    "            k=K_train,\n",
    "            kapprox=K_approx_train,\n",
    "        ),\n",
    "        get_stats(\n",
    "            x=X_test,\n",
    "            xr=Xr_test,\n",
    "            y=Y_test,\n",
    "            t=T_KPCA_test,\n",
    "            k=K_test_test,\n",
    "            kapprox=K_approx_test,\n",
    "        ),\n",
    "    ],\n",
    "    headers=[\"Training\", \"Testing\"],\n",
    "    title=\"KPCA\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
