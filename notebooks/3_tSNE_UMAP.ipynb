{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook, we introduce t-SNE, a probability-distribution preserving dimensionality reduction technique, as well as UMAP, a technique which yields a similar result in a short time.\n",
    "\n",
    "**Note**: To display values of variables in the markdown cells, you should [enable the jupyter contrib nbextension](https://stackoverflow.com/questions/52812231/print-variable-in-jupyter-notebook-markdown-cell-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append(\"../\")\n",
    "from utilities.general import load_variables, sorted_eig, get_stats\n",
    "from utilities.plotting import (\n",
    "    plot_projection,\n",
    "    plot_regression,\n",
    "    check_mirrors,\n",
    "    get_cmaps,\n",
    "    table_from_dict,\n",
    ")\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel\n",
    "from functools import partial\n",
    "\n",
    "from skcosmo.preprocessing import KernelNormalizer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from openTSNE import TSNE\n",
    "import umap\n",
    "from functools import partial\n",
    "\n",
    "from skcosmo.preprocessing import KernelNormalizer\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use(\"../utilities/kernel_pcovr.mplstyle\")\n",
    "dbl_fig = (2 * plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb). As mentioned in the [foreword notebook](0_Foreword.ipynb), pre-computed features should be downloaded from [here](https://www.dropbox.com/s/itokckbbkvxaqsk/precomputed.npz?dl=0) and the file `precomputed.npz` should be copied to the `datasets/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of [t-distribution stochastic neighbor embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (t-SNE) [[1]](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), is to reproduce the probability distribution of the original feature space. Preserving the probability distribution of feature space can be particularly important in simulations when modeling or observing thermodynamic quantities, as one might use a low-dimensional map to understand the local landscape.\n",
    "We start from the conditional probability distribution that points $j$ is a neighbor to points $i$ and assuming an underlying Gaussian distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(j | i) =\\frac{\\exp[d(x_i, x_j)/2\\sigma^2]}{sum_{k\\neq i}{exp[d(x_i, x_j)/2\\sigma^2]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\sigma$ is our Gaussian kernel width, and $d(x_i, x_j)$ is a distance metric. This distance metric is often, but not necessarily, the Euclidean distance $d(x_i, x_j) = || x_i - x_j||^2$.  The probability distribution $p_{ij}$ is then given by the normalized sum of the conditional probabilities using $p(j|i)$ and $p(i|j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then map $p_{ij}$ onto a lower-dimension distribution $q_{ij}$ by minimizing the [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) divergence between the two distributions.\n",
    "\n",
    "$$KL(p, q) = sum_{ij} \\left[p_{ij}log\\left(\\frac{p_{ij}}{q_{ij}}\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many packages for computing t-SNE (including a [package available](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) in scikit-learn), here we use [openTSNE](https://opentsne.readthedocs.io/en/latest/) for its computational efficiency and its focus on algorithm convergence.\n",
    "\n",
    "For readers wishing to know more about the ins and outs of t-SNE, there is an excellent article written by [Wattenberg, et al.](https://distill.pub/2016/misread-tsne/) that steps through the importance of each hyperparameter very nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE vs. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    n_components=2, # dimensionality of q\n",
    "    perplexity=10, # how to balance between local neighborhoods and global shape\n",
    "    metric=\"euclidean\", # distance metric\n",
    "    n_jobs=2, # parallelization\n",
    "    random_state=42, # random seed\n",
    ")\n",
    "T_tsne_train=tsne.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12,4), gridspec_kw=dict(width_ratios=(1, 1.5)))\n",
    "plot_projection(\n",
    "    numbers[i_train], T_tsne_train, title=\"t-SNE of Training Data\", \n",
    "    x_label=\"t-SNE 1\",\n",
    "    y_label=\"t-SNE 2\",\n",
    "    cbar=True,\n",
    "    cbar_title=\"Atomic\\nNumber\",\n",
    "    fig=fig,\n",
    "    ax=ax[1],\n",
    "    s=10,\n",
    "    **cmaps\n",
    ")\n",
    "ax[1].set_aspect('auto')\n",
    "\n",
    "plot_projection(\n",
    "    numbers[i_train], PCA(n_components=2).fit_transform(X_train), title=\"PCA of Training Data\", \n",
    "    cbar=False,\n",
    "    fig=fig,\n",
    "    ax=ax[0],\n",
    "    s=10,\n",
    "    **cmaps\n",
    ")\n",
    "ax[0].set_xticks([])\n",
    "ax[1].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[1].set_yticks([])\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the t-SNE projection differs from the PCA. We still get the separation of data points due to chemical identity; however, the clusters of hydrogens and carbons are much more stretched out, likely due to the greater variability of atomic environments for these two atomic species. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not necessary to use the euclidean distance in t-SNE, and we can similarly modify it to incorporate non-linearity like we did PCA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "\n",
    "Uniform Manifold Approximation and Projection (UMAP) is a dimensionality reduction technique, introduced first by [McInnes and Healy in 2018](https://joss.theoj.org/papers/10.21105/joss.00861), aimed to recreate the underlying manifold of samples in a lower-dimensional space. UMAP will obtain a visually similar result to t-SNE, but uses tricks of topological data analyses to drastically reduce the computational overhead. Given the complexity of these mathematics, the implementation of UMAP is beyond the scope of this text, but we urge interested readers to dive into the derivation found in [McInnes and Healy in 2018](https://joss.theoj.org/papers/10.21105/joss.00861) or in the online documentation to the [UMAP software package](https://umap-learn.readthedocs.io). \n",
    "\n",
    "In UMAP, we first assume that all data points lie uniformly distributed on a high-dimensional manifold. On this manifold, we extend a radius from each point. Any two points with overlapping radii are assumed to be neighbors, and each point is enforced to have a minimum number of neighbors. In UMAP, we assume these radii to be “fuzzy”, such that the likelihood of being neighbors decreases with distance from each point. This procedure results in a graph of points; the resulting low-dimensional embedding is that which best preserves this graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neigh = np.logspace(0, 2, 5, dtype=int)\n",
    "n_neigh[0] = 2 # cannot have only one neighbor\n",
    "\n",
    "fig, axes = plt.subplots(1, len(n_neigh),figsize=(2*len(n_neigh),2))\n",
    "\n",
    "for n, ax in zip(n_neigh, axes):\n",
    "    u = umap.UMAP(n_neighbors=n).fit_transform(X_train)\n",
    "    plot_projection(\n",
    "                    numbers[i_train], u, \n",
    "                    title=f\"n={n}\", \n",
    "                    x_label=\"UMAP 1\",\n",
    "                    y_label=\"UMAP 2\",\n",
    "                    cbar=False,\n",
    "                    fig=fig,\n",
    "                    ax=ax,\n",
    "                    s=10,\n",
    "                    **cmaps\n",
    "                    )\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of neighbors increases, the clusters separate further. This is because the clusters are each becoming self-isolated as the neighborhoods are more clearly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
